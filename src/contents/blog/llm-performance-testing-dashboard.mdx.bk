---
title: 'Build LLM Performance Testing Dashboard with KV Cache Monitoring on DigitalOcean'
publishedAt: '2025-06-10'
description: >-
  Learn how to deploy a comprehensive LLM API scalability testing dashboard with real-time KV Cache monitoring. This tutorial covers dashboard setup from GitHub repository, Prometheus metrics collection, Grafana visualization, and performance benchmarking for production LLM deployments.
banner: 'llm-dashboard-banner'
tags: 'llm, performance, monitoring, dashboard'
---

# Build LLM Performance Testing Dashboard with KV Cache Monitoring on DigitalOcean

This tutorial will guide you through deploying a complete LLM API scalability testing dashboard with integrated Dynamo KV Cache monitoring system. Using our pre-built GitHub repository, you can quickly establish efficient LLM performance monitoring and testing platforms in production environments.

**â±ï¸ Estimated Deployment Time: 30-45 minutes**

**ðŸ“‹ Prerequisites: This tutorial assumes you have completed the [NVIDIA Dynamo deployment tutorial](/blog/nvidia-dynamo-digitalocean-tutorial) and have a running Dynamo LLM service.**

---

## Overview

This **LLM Performance Testing Dashboard** provides a comprehensive solution for monitoring and testing LLM API performance, featuring:

1. **Real-time KV Cache Monitoring**  
   Track cache hit rates, block utilization, and memory efficiency through embedded Grafana dashboards.

2. **Load Testing Interface**  
   User-friendly Next.js dashboard for conducting systematic performance tests with diverse question sets.

3. **Metrics Collection Pipeline**  
   Complete monitoring stack with Prometheus, Grafana, and custom metrics aggregation.

4. **Production-Ready Components**  
   Pre-built dashboard with advanced features like concurrent testing, real-time results, and performance analytics.

---

## System Architecture

```
ðŸ“¦ Complete Architecture:
Next.js Dashboard (Port 3050) â†’ Load Testing Interface
         â†“
LLM API (Port 8000) â†’ DeepSeek-R1-Distill-Llama-8B
         â†“
Dynamo KV Cache â†’ Optimized Inference Performance
         â†“
Metrics Collection (Port 9091) â†’ KV Cache Metrics Aggregation
         â†“
Prometheus (Port 9090) â†’ Metrics Storage
         â†“
Grafana (Port 3000) â†’ Visualization & Monitoring
```

---

# Tutorial Steps

## Step 1: Verify Dynamo Service Status

> Why: Ensure the foundation LLM service is running properly before building the monitoring dashboard.

```bash
# Check Dynamo container status
docker ps | grep dynamo

# Verify LLM API health
curl http://localhost:8000/health

# Test basic inference
curl localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false,
    "max_tokens": 50
  }'
```

## Step 2: Set Up Metrics Collection Service

> Why: The metrics aggregation service is the critical bridge between Dynamo KV Cache and Prometheus monitoring.

```bash
# Navigate to Dynamo workspace
docker exec -it <dynamo_container_id> bash
cd /workspace

# Start metrics aggregation service (Critical Step)
./target/release/metrics --component VllmWorker --endpoint load_metrics --host 0.0.0.0 --port 9091 &

# Verify metrics endpoint
curl http://localhost:9091/metrics | grep llm_kv
```

## Step 3: Deploy Monitoring Stack

> Why: Set up Prometheus and Grafana for metrics collection and visualization.

Create `docker-compose.monitoring.yml`:

```yaml
version: '3.8'
services:
  prometheus:
    image: prom/prometheus:latest
    network_mode: 'host'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    network_mode: 'host'
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

Create `prometheus.yml`:

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'dynamo-metrics'
    static_configs:
      - targets: ['localhost:9091']
    scrape_interval: 5s
```

```bash
# Start monitoring stack
docker compose -f docker-compose.monitoring.yml up -d

# Verify services
curl http://localhost:9090  # Prometheus
curl http://localhost:3000  # Grafana (admin/admin)
```

## Step 4: Configure Grafana for Dashboard Integration

> Why: Enable iframe embedding and anonymous access for seamless dashboard integration.

```bash
# Enable iframe embedding in Grafana
docker exec -u root <grafana_container_id> sh -c "sed -i 's/;allow_embedding = false/allow_embedding = true/' /etc/grafana/grafana.ini"

# Enable anonymous access for embedded dashboards
docker exec -u root <grafana_container_id> sh -c "sed -i 's/;enabled = false/enabled = true/' /etc/grafana/grafana.ini"

# Restart Grafana to apply changes
docker restart <grafana_container_id>

# Verify Grafana configuration
curl http://localhost:3000/api/health
```

## Step 5: Clone and Set Up Dashboard Repository

> Why: Use the pre-built, production-ready dashboard with advanced features and optimizations.

```bash
# Clone the LLM Performance Testing Dashboard
git clone https://github.com/iambigmomma/LLM-API-Scalability-Testing-Dashboard.git
cd LLM-API-Scalability-Testing-Dashboard

# Install dependencies
npm install
```

## Step 6: Configure Dashboard Settings

> Why: Update server IP configuration to match your DigitalOcean Droplet environment.

```bash
# Edit the configuration file
nano src/config/dashboard-config.ts
```

Update the `SERVER_IP` value:

```typescript
export const config = {
  SERVER_IP: 'YOUR_DROPLET_IP', // Replace with your DigitalOcean Droplet IP
  GRAFANA_PORT: 3000,
  DASHBOARD_PORT: 3050,
  LLM_API_PORT: 8000,
  PROMETHEUS_PORT: 9090,
  METRICS_PORT: 9091,
};

export const getGrafanaBaseUrl = () =>
  `http://${config.SERVER_IP}:${config.GRAFANA_PORT}`;
export const getLLMApiUrl = () =>
  `http://${config.SERVER_IP}:${config.LLM_API_PORT}`;
export const getPrometheusUrl = () =>
  `http://${config.SERVER_IP}:${config.PROMETHEUS_PORT}`;
```

## Step 7: Create Grafana Dashboard for LLM Metrics

> Why: Set up visual monitoring for KV Cache metrics and system performance.

1. **Access Grafana**: Navigate to `http://YOUR_DROPLET_IP:3000` (admin/admin)
2. **Add Prometheus Data Source**:
   - Go to Configuration â†’ Data Sources
   - Add Prometheus with URL: `http://localhost:9090`
3. **Create LLM Worker Metrics Dashboard**:
   - Create new dashboard with ID: `llm-worker-metrics`
   - Add panels for key metrics:

```json
{
  "dashboard": {
    "id": null,
    "title": "LLM Worker Metrics",
    "uid": "llm-worker-metrics",
    "panels": [
      {
        "title": "KV Cache Hit Rate",
        "type": "stat",
        "targets": [{ "expr": "llm_kv_hit_rate_percent" }],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100
          }
        }
      },
      {
        "title": "Active KV Blocks",
        "type": "timeseries",
        "targets": [{ "expr": "llm_kv_blocks_active" }]
      },
      {
        "title": "Request Latency (95th percentile)",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          }
        ]
      }
    ]
  }
}
```

## Step 8: Start Dashboard Application

> Why: Launch the complete testing interface for conducting performance evaluations.

```bash
# Start Next.js dashboard on port 3050
npm run dev -- --port 3050

# Verify dashboard access
curl http://localhost:3050
```

## Step 9: Conduct Performance Testing

> Why: Validate system performance under different load conditions and gather baseline metrics.

1. **Access Dashboard**: Navigate to `http://YOUR_DROPLET_IP:3050`

2. **Dashboard Features**:

   - **Load Testing Panel**: Configure concurrent requests (1-50), total requests (10-1000), and batch intervals
   - **Diverse Test Questions**: Toggle between custom messages and curated question sets
   - **Real-time Results**: Live feed of API responses with expandable answers
   - **Performance Analytics**: Success rates, latency distribution, and throughput metrics
   - **Embedded Grafana**: Real-time KV Cache monitoring with auto-refresh

3. **Recommended Test Configuration**:
   - Start with 5 concurrent requests
   - Use diverse test questions for realistic scenarios
   - Monitor KV Cache metrics during testing
   - Gradually increase load to find performance limits

## Step 10: Validate Complete Monitoring Pipeline

> Why: Ensure all components are working together and collecting accurate metrics.

```bash
# Check metrics collection
curl http://localhost:9091/metrics | grep -E "(llm_kv|http_request)"

# Verify Prometheus targets
curl http://localhost:9090/api/v1/targets

# Test Grafana API and dashboard
curl -u admin:admin http://localhost:3000/api/health
curl http://localhost:3000/d/llm-worker-metrics

# Verify dashboard functionality
curl http://localhost:3050
```

ðŸŽ‰ **Congratulations!** You've successfully deployed a comprehensive LLM Performance Testing Dashboard with real-time KV Cache monitoring. Your production-ready system includes advanced load testing capabilities, real-time metrics visualization, and performance analytics!

---

## Dashboard Features Overview

### ðŸŽ¯ Load Testing Capabilities

- **Concurrent Testing**: Support for 1-50 parallel requests
- **Batch Control**: Configurable intervals between request batches (0-2000ms)
- **Diverse Questions**: 50+ curated test questions across different domains
- **Custom Messages**: Support for specific test scenarios
- **Parameter Control**: Temperature (0.0-2.0) and max tokens (50-1000) adjustment

### ðŸ“Š Real-time Analytics

- **Live Results Feed**: Last 50 API responses with expandable answers
- **Performance Metrics**: Success rate, average/min/max latency, requests per second
- **Color-coded Indicators**: Visual success/failure status
- **Timestamp Tracking**: Detailed request timing information

### ðŸ“ˆ Monitoring Integration

- **Embedded Grafana**: Full dashboard integration with auto-refresh (2s intervals)
- **Direct Links**: Quick access to Grafana interface and home page
- **Error Handling**: Graceful fallback when monitoring services are unavailable
- **Real-time Updates**: Live KV Cache metrics and system performance indicators

---

## Key Performance Metrics

### KV Cache Monitoring

| Metric                      | Description               | Optimal Range  |
| --------------------------- | ------------------------- | -------------- |
| `llm_kv_hit_rate_percent`   | Cache hit rate percentage | > 80%          |
| `llm_kv_blocks_active`      | Active cache blocks       | < 70% of total |
| `llm_requests_active_slots` | Active request slots      | < 80% of total |

### Performance Benchmarks

- **Low Load (< 30% KV utilization)**: Normal operation
- **Medium Load (30-70% KV utilization)**: Monitor closely
- **High Load (> 70% KV utilization)**: Consider optimization
- **Cache Hit Rate > 80%**: Excellent performance
- **Request Success Rate > 95%**: Production ready

---

## Common Issues and Solutions

### Issue 1: Dashboard Configuration

**Problem**: Dashboard shows connection errors or incorrect URLs
**Solution**: Verify `SERVER_IP` in `src/config/dashboard-config.ts` matches your Droplet IP

### Issue 2: Grafana Integration Issues

**Problem**: Embedded Grafana dashboard not loading
**Solution**:

```bash
# Ensure iframe embedding is enabled
docker exec -u root <grafana_container> sh -c "grep 'allow_embedding' /etc/grafana/grafana.ini"
# Should show: allow_embedding = true
```

### Issue 3: Missing KV Cache Metrics

**Problem**: Grafana shows "No data" for KV metrics
**Solution**: Ensure metrics aggregation service is running on port 9091

```bash
curl http://localhost:9091/metrics | grep llm_kv
```

### Issue 4: Network Access Issues

**Problem**: Dashboard not accessible from external networks
**Solution**:

- Verify firewall allows port 3050
- Check DigitalOcean Droplet firewall settings
- Ensure `SERVER_IP` configuration is correct

---

## Advanced Usage Tips

### ðŸ”§ Customization Options

- **Test Questions**: Edit `src/data/testQuestions.ts` to add domain-specific questions
- **UI Themes**: Modify Tailwind CSS classes for custom styling
- **Metrics Display**: Adjust Grafana dashboard panels for specific monitoring needs
- **API Parameters**: Configure default temperature and token limits in dashboard settings

### ðŸ“ˆ Performance Optimization

- **Batch Intervals**: Use 100-500ms intervals for sustained load testing
- **Concurrent Limits**: Start with 5-10 concurrent requests, scale based on system capacity
- **Question Diversity**: Enable diverse questions for realistic cache behavior testing
- **Monitoring Frequency**: Adjust Grafana refresh rates based on testing duration

---

## Next Steps

With your LLM Performance Testing Dashboard operational, consider these advanced optimizations:

#### **Production Deployment Enhancements**

- **Multi-Model Testing**: Compare different LLM models side-by-side
- **Automated Testing**: Schedule regular performance tests with CI/CD integration
- **Alert Configuration**: Set up Grafana alerts for performance threshold breaches
- **Historical Analysis**: Configure long-term metrics storage for trend analysis

Understanding these performance characteristics through systematic testing will help you optimize your LLM deployment for production workloads and make informed scaling decisions.

---

## Conclusion

You have successfully deployed a production-ready LLM Performance Testing Dashboard using the [GitHub repository](https://github.com/iambigmomma/LLM-API-Scalability-Testing-Dashboard). This comprehensive platform enables you to:

- Conduct systematic load testing with advanced configuration options
- Monitor KV Cache performance in real-time through embedded Grafana dashboards
- Analyze detailed performance metrics and identify optimization opportunities
- Make data-driven decisions for scaling and resource allocation

The pre-built dashboard provides enterprise-grade features while maintaining ease of use, making it perfect for both development and production environments.

**Happy testing and optimizing!**
